# LiteLLM configuration
app:
  name: litellm
  image:
    repository: ghcr.io/berriai/litellm-database
    tag: "main-stable"
    pullPolicy: Always
  
  replicas: 1
  port: 4000

  resources:
    requests:
      memory: "3Gi"
      cpu: "1500m"
    limits:
      memory: "6Gi"
      cpu: "3000m"

# Service configuration
service:
  name: litellm-service
  type: ClusterIP
  port: 4000
  targetPort: 4000

# ServiceAccount configuration
serviceAccount:
  enabled: true
  name: litellm-sa
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::901779868336:role/AmazonEKS_LiteLLM_Role"

# ConfigMap for model configuration
config:
  models:
    # Task model - Chat completions (using inference profile)
    - model_name: meta-llama/Llama-3.2-3B-Instruct
      litellm_params:
        model: bedrock/eu.meta.llama3-2-3b-instruct-v1:0
        aws_region_name: eu-central-1
    # Embedding model (ON_DEMAND - no inference profile needed)
    - model_name: Cohere/Cohere-embed-multilingual-v3.0
      litellm_params:
        model: bedrock/cohere.embed-multilingual-v3
        aws_region_name: eu-central-1
    # Reranking model (ON_DEMAND - no inference profile needed)
    - model_name: Cohere/rerank-v3.5
      litellm_params:
        model: bedrock/cohere.rerank-v3-5:0
        aws_region_name: eu-central-1
    # Apertus 8B models
    # - model_name: swiss-ai/apertus-8b-instruct
    #   litellm_params:
    #     model: openai/apertus-8b-instruct
    #     api_base: http://llm-services-router-service.llm-services.svc.cluster.local/v1
    #     api_key: "os.environ/VLLM_API_KEY"
    #     weight: 2
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-Instruct-2509
        api_base: https://api-internal-exoscale.publicai.co/v1
        api_key: "os.environ/VLLM_API_KEY_EXOSCALE"
        weight: 1
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-Instruct-2509
        api_base: https://api-internal-exoscale-2.publicai.co/v1
        api_key: "os.environ/VLLM_API_KEY_EXOSCALE"
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 8B model via CSCS endpoint
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-Instruct-2509
        api_base: https://vllm.svc.cscs.ch/apertus-70b-instruct/v1
        api_key: "os.environ/VLLM_API_KEY_CSCS"
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 8B model via AISG endpoint
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-Instruct-2509
        api_base: https://api.sea-lion.ai/v1
        api_key: "os.environ/SEALION_API_KEY"
        weight: 1
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 8B model via ANU endpoint
    - model_name: swiss-ai/apertus-8b-instruct-anu
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-2509
        api_base: https://apertus-01.nci.org.au/v1
        api_key: "os.environ/VLLM_API_KEY_ANU"
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 70B model via AISG endpoint
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-70B-Instruct-2509
        api_base: https://api.sea-lion.ai/v1
        api_key: "os.environ/SEALION_API_KEY"
        weight: 1
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 70B model via vLLM router service
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/apertus-70b-instruct
        api_base: http://llm-services-router-service.llm-services.svc.cluster.local/v1
        api_key: "os.environ/VLLM_API_KEY"
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 70B model via CUDO endpoint
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-70B-Instruct-2509
        api_base: https://api-internal-cudo.publicai.co/v1
        api_key: "os.environ/VLLM_API_KEY_CUDO"
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # Apertus 70B model via CSCS endpoint
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-70B-Instruct-2509
        api_base: https://vllm.svc.cscs.ch/apertus-70b-instruct/v1
        api_key: "os.environ/VLLM_API_KEY_CSCS"
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
     # Apertus 70B model via ANU endpoint
    - model_name: swiss-ai/apertus-70b-instruct-anu
      litellm_params:
        model: openai/swiss-ai/Apertus-70B-2509
        api_base: https://apertus-02.nci.org.au/v1
        api_key: "os.environ/VLLM_API_KEY_ANU"
        temperature: 0.8
        top_p: 0.9
        max_tokens: 8192
    # SeaLion model
    - model_name: aisingapore/Gemma-SEA-LION-v4-27B-IT
      litellm_params:
        model: openai/aisingapore/Gemma-SEA-LION-v4-27B-IT
        api_base: https://api.sea-lion.ai
        api_key: "os.environ/SEALION_API_KEY"

  # guardrails:
  #   - guardrail_name: "bedrock-pre-guard"
  #     litellm_params:
  #       guardrail: bedrock
  #       mode: "during_call"
  #       guardrailIdentifier: ux3n95in9v9h
  #       guardrailVersion: "DRAFT"
  #       aws_region_name: "eu-central-1"
  #       disable_exception_on_block: true
  #       default_on: true

# Secrets configuration
secrets:
  name: litellm-secrets
  litellmMasterKey: ""
  litellmSaltKey: ""
  databaseUrl: ""
  redisHost: ""
  redisUrl: ""
  togetherApiKey: ""
  sealionApiKey: ""
  vllmApiKey: ""
  vllmApiKeyExoscale: ""
  vllmApiKeyAnu: ""
  vllmApiKeyCscs: ""
  vllmApiKeyCudo: ""

# Environment variables
env:
  aws_region: "eu-central-1"
  litellmRedisPort: "6379"
  litellmRedisNamespace: "litellm"

# Router configuration
router:
  routingStrategy: "simple-shuffle"

# Autoscaling configuration
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 8
  targetCPUUtilizationPercentage: 60
  targetMemoryUtilizationPercentage: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60