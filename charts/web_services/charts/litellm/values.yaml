# LiteLLM configuration
app:
  name: litellm
  image:
    # repository: ghcr.io/berriai/litellm-database
    # tag: "main-stable"
    repository: ghcr.io/forpublicai/litellm-database
    tag: spend_logs_patch 
    pullPolicy: Always
  
  replicas: 1
  port: 4000

  resources:
    requests:
      memory: "2Gi"
      cpu: "300m"
    limits:
      memory: "4Gi"
      cpu: "1000m"

# Service configuration
service:
  name: litellm-service
  type: ClusterIP
  port: 4000
  targetPort: 4000

# ServiceAccount configuration
serviceAccount:
  enabled: true
  name: litellm-sa
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::901779868336:role/AmazonEKS_LiteLLM_Role"

# ConfigMap for model configuration
config:
  litellm_settings:
    cache: true
    cache_params:
      type: redis
      namespace: "litellm.caching"
      ttl: 600  # 10 minutes default TTL
      # Redis SSL is configured via REDIS_URL environment variable

  models:
    # Task model - Chat completions (using inference profile)
    - model_name: meta-llama/Llama-3.2-3B-Instruct
      litellm_params:
        model: bedrock/eu.meta.llama3-2-3b-instruct-v1:0
        aws_region_name: eu-central-1
      model_info:
        input_cost_per_token: 0.00000015  # $0.15 per 1M tokens (estimated)
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens (estimated)
    # Embedding model (ON_DEMAND - no inference profile needed)
    - model_name: Cohere/Cohere-embed-multilingual-v3.0
      litellm_params:
        model: bedrock/cohere.embed-multilingual-v3
        aws_region_name: eu-central-1
      model_info:
        input_cost_per_token: 0.0000001  # $0.10 per 1M tokens
    # Reranking model (ON_DEMAND - no inference profile needed)
    - model_name: Cohere/rerank-v3.5
      litellm_params:
        model: bedrock/cohere.rerank-v3-5:0
        aws_region_name: eu-central-1
      model_info:
        input_cost_per_token: 0.000001  # $1.00 per 1M tokens (query-based pricing)
    # Olmo 7B model think via parascale endpoint
    - model_name: allenai/Olmo-3-7B-Instruct
      litellm_params:
        model: openai/allenai/Olmo-3-7B-Instruct
        api_base: https://api.parasail.io/v1
        api_key: "os.environ/PARASCALE_API_KEY"
        weight: 1
        temperature: 0.6
        top_p: 0.95
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000010  # $0.10 per 1M tokens (parasail-olmo-3-7b-instruct)
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # # Olmo 7B model think via parascale endpoint
    # - model_name: allenai/Olmo-3-7B-Think
    #   litellm_params:
    #     model: openai/Olmo-3-7B-Think
    #     api_base: https://api.parasail.io/v1
    #     api_key: "os.environ/PARASCALE_API_KEY"
    #     temperature: 0.6
    #     top_p: 0.95
    #     max_tokens: 16384
    # Olmo 32B model think via parascale endpoint
    - model_name: allenai/Olmo-3.1-32B-Think
      litellm_params:
        model: openai/allenai/Olmo-3.1-32B-Think
        api_base: https://api.parasail.io/v1
        api_key: "os.environ/PARASCALE_API_KEY"
        temperature: 0.6
        top_p: 0.95
        max_tokens: 32768
      model_info:
        input_cost_per_token: 0.00000030  # $0.30 per 1M tokens (parasail-olmo-3.1-32b-think)
        output_cost_per_token: 0.00000055  # $0.55 per 1M tokens
    # Olmo 32B model instruct via cirrascale endpoint
    - model_name: allenai/Olmo-3.1-32B-Instruct
      litellm_params:
        model: openai/Olmo-3.1-32B-Instruct
        api_base: https://ai2endpoints.cirrascale.ai/api
        api_key: "os.environ/CIRRASCALE_API_KEY"
        weight: 1
        temperature: 0.6
        top_p: 0.95
        max_tokens: 32768
      model_info:
        input_cost_per_token: 0.00000005  # $0.05 per 1M tokens (cirrascale)
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # Olmo 7B model think via cirrascale endpoint
    - model_name: allenai/Olmo-3-7B-Think
      litellm_params:
        model: openai/Olmo-3-7B-Think
        api_base: https://ai2endpoints.cirrascale.ai/api
        api_key: "os.environ/CIRRASCALE_API_KEY"
        weight: 1
        temperature: 0.6
        top_p: 0.95
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000007  # $0.07 per 1M tokens (cirrascale)
        output_cost_per_token: 0.00000015  # $0.15 per 1M tokens
    # EuroLLM 22B model instruct via intel endpoint
    - model_name: utter-project/EuroLLM-22B-Instruct-2512
      litellm_params:
        model: openai/utter-project/EuroLLM-22B-Instruct-2512
        api_base: https://api-internal-intel.publicai.co/eurollm/v1
        api_key: "os.environ/VLLM_API_KEY_INTEL"
        weight: 1
        temperature: 0.6
        top_p: 0.95
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000010  # $0.10 per 1M tokens
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # Olmo 7B model instruct via intel endpoint
    - model_name: allenai/Olmo-3-7B-Instruct
      litellm_params:
        model: openai/allenai/Olmo-3-7B-Instruct
        api_base: https://api-internal-intel.publicai.co/olmo/v1
        api_key: "os.environ/VLLM_API_KEY_INTEL"
        weight: 1
        temperature: 0.6
        top_p: 0.95
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000010  # $0.10 per 1M tokens
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # Apertus 8B model via intel endpoint
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/swiss-ai/Apertus-8B-Instruct-2509
        api_base: https://api-internal-intel.publicai.co/apertus/v1
        api_key: "os.environ/VLLM_API_KEY_INTEL"
        supports_vision: true  
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000010  # $0.10 per 1M tokens
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # Apertus 8B model via CSCS endpoint
    - model_name: swiss-ai/apertus-8b-instruct
      litellm_params:
        model: openai/Apertus-8B-Instruct-2509
        api_base: https://llm-proxy.svc.cscs.ch/v1
        api_key: "os.environ/VLLM_API_KEY_CSCS"
        supports_vision: true  
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000010  # $0.10 per 1M tokens
        output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    # Apertus 70B model via Infomaniak endpoint
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/Apertus-70B-Instruct-2509
        api_base: https://api.infomaniak.com/2/ai/106744/openai/v1
        api_key: "os.environ/INFOMANIAK_API_KEY"
        supports_vision: true  
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000082  # $0.82 per 1M tokens (estimated from 0.70 EUR per M tokens)
        output_cost_per_token: 0.00000292  # $2.92 per 1M tokens (estimated from 2.50 EUR per M tokens)
    # Apertus 70B model via CSCS endpoint
    - model_name: swiss-ai/apertus-70b-instruct
      litellm_params:
        model: openai/Apertus-70B-Instruct-2509
        api_base: https://llm-proxy.svc.cscs.ch/v1
        api_key: "os.environ/VLLM_API_KEY_CSCS"
        supports_vision: true  
        weight: 20
        temperature: 0.8
        top_p: 0.9
        max_tokens: 16384
      model_info:
        input_cost_per_token: 0.00000030  # $0.30 per 1M tokens
        output_cost_per_token: 0.00000060  # $0.60 per 1M tokens
    # SeaLion model (Gemma)
    - model_name: aisingapore/Gemma-SEA-LION-v4-27B-IT
      litellm_params:
        model: openai/aisingapore/Gemma-SEA-LION-v4-27B-IT
        api_base: https://api.sea-lion.ai
        api_key: "os.environ/SEALION_API_KEY"
        supports_vision: true
      model_info:
        input_cost_per_token: 0.00000020  # $0.20 per 1M tokens (estimated for 27B model)
        output_cost_per_token: 0.00000040  # $0.40 per 1M tokens
    # SeaLion model (Qwen)
    - model_name: aisingapore/Qwen-SEA-LION-v4-32B-IT
      litellm_params:
        model: openai/aisingapore/Qwen-SEA-LION-v4-32B-IT
        api_base: https://api.sea-lion.ai
        api_key: "os.environ/SEALION_API_KEY"
        supports_vision: true
      model_info:
        input_cost_per_token: 0.00000025  # $0.25 per 1M tokens (estimated for 32B model)
        output_cost_per_token: 0.00000050  # $0.50 per 1M tokens
    # # Spanish Models
    # - model_name: BSC-LT/ALIA-40b-instruct_Q8_0
    #   litellm_params:
    #     model: openai//root/.cache/huggingface/ALIA-40b-instruct_Q8_0/ALIA-40b-instruct_bos_Q8_0.gguf
    #     api_base: https://api-internal-cudo.publicai.co/alia/v1
    #     api_key: "os.environ/VLLM_API_KEY_CUDO"
    #     temperature: 0.8
    #     top_p: 0.9
    #     max_tokens: 16384
    #   model_info:
    #     input_cost_per_token: 0.00000025  # $0.25 per 1M tokens (estimated for 40B model)
    #     output_cost_per_token: 0.00000050  # $0.50 per 1M tokens
    # - model_name: BSC-LT/salamandra-7b-instruct-tools-16k
    #   litellm_params:
    #     model: openai/BSC-LT/salamandra-7b-instruct-tools-16k
    #     api_base: https://api-internal-cudo.publicai.co/salamandra/v1
    #     api_key: "os.environ/VLLM_API_KEY_CUDO"
    #     temperature: 0.8
    #     top_p: 0.9
    #     max_tokens: 16384
    #   model_info:
    #     input_cost_per_token: 0.00000010  # $0.10 per 1M tokens (estimated for 7B model)
    #     output_cost_per_token: 0.00000020  # $0.20 per 1M tokens
    - model_name: mistralai/mistral-small-3-1
      litellm_params:
        model: openai/mistral-small-3-1
        api_base: https://api.compactif.ai/v1
        api_key: "os.environ/MULTIVERSE_API_KEY"
        temperature: 0.15
        top_p: 0.85
        max_tokens: 32000
      model_info:
        input_cost_per_token: 0.00000011  # $0.11 per 1M tokens 
        output_cost_per_token: 0.00000017  # $0.17 per 1M tokens
    - model_name: dicta-il/DictaLM-3.0-24B-Thinking
      litellm_params:
        model: openai/DictaLM-3.0-24B-Thinking
        api_base: https://dictalm3-0-api-backend.loadbalancer3.dicta.org.il/v1
        api_key: "os.environ/DICTA_API_KEY"
        temperature: 0.15
        top_p: 0.85
        max_tokens: 16000
      model_info:
        input_cost_per_token: 0.00000020  # $0.20 per 1M tokens 
        output_cost_per_token: 0.00000040  # $0.40 per 1M tokens
  # guardrails:
  #   - guardrail_name: "bedrock-pre-guard"
  #     litellm_params:
  #       guardrail: bedrock
  #       mode: "during_call"
  #       guardrailIdentifier: ux3n95in9v9h
  #       guardrailVersion: "DRAFT"
  #       aws_region_name: "eu-central-1"
  #       disable_exception_on_block: true
  #       default_on: true

# Secrets configuration
secrets:
  name: litellm-secrets
  litellmMasterKey: ""
  litellmSaltKey: ""
  databaseUrl: ""
  redisHost: ""
  redisUrl: ""
  togetherApiKey: ""
  sealionApiKey: ""
  vllmApiKey: ""
  vllmApiKeyExoscale: ""
  vllmApiKeyAnu: ""
  vllmApiKeyCscs: ""
  vllmApiKeyCudo: ""
  vllmApiKeyHh: ""
  vllmApiKeyIntel: ""
  cirrascaleApiKey: ""
  parascaleApiKey: ""
  multiverseApiKey: ""
  lagoApiKey: ""  # Lago API key for billing integration
  dictaApiKey: ""

# Environment variables
env:
  aws_region: "eu-central-1"
  litellmRedisPort: "6379"
  litellmRedisNamespace: "litellm"

# Router configuration
router:
  routingStrategy: "simple-shuffle"

# Autoscaling configuration
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 75
  targetMemoryUtilizationPercentage: 85
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

# Lago billing integration
lago:
  enabled: true  # Set to true to enable Lago billing
  eventCode: "public_ai_models"  # Event code in Lago for billing metrics
  chargeBy: "end_user_id"  # Charge by: user_id, team_id, or end_user_id (uses the 'user' parameter from API calls)
  apiBase: "https://lago-api.publicai.co"  # Lago API base URL. Not being used as Litellm communicates internally to Lago.