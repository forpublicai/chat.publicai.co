# Global values for llm-services chart
global:
  namespace: llm-services
  
  # AWS configuration
  aws:
    region: eu-central-2
    bucketName: publicai-bucket
    bucketPrefix: models/

# Karpenter GPU node pool configuration
nodePool:
  enabled: true
  name: gpu
  disruption:
    budgets:
    - nodes: "10%"
    consolidateAfter: 1h
    consolidationPolicy: WhenEmpty
  
  requirements:
    capacityType: "on-demand"  # Change to "capacity-block" for Capacity Blocks
    architecture: "amd64"
    instanceTypes:
      - "p4d.24xlarge"  # TODO: Adapt to p instances once available
  
  taints:
    - key: nvidia.com/gpu
      effect: NoSchedule
  
  terminationGracePeriod: 24h0m0s

# S3 storage configuration
s3Storage:
  enabled: true
  persistentVolume:
    name: s3-pv
    capacity: 1200Gi
    mountOptions:
      region: eu-central-2  # Will use global.aws.region if not specified
      prefix: models/      # Will use global.aws.bucketPrefix if not specified
    bucketName: publicai-bucket  # Will use global.aws.bucketName if not specified
  
  persistentVolumeClaim:
    name: s3-pvc
    capacity: 1200Gi

vllm:
  enabled: true

# Values for vllm-stack subchart  
vllm-stack:
  servingEngineSpec:
    runtimeClassName: ""
    vllmApiKey: "" # Will be set via --set from llm.sh script
    
    modelSpec:
    # Large model: 70B Apertus (4 GPUs - factor of 64 for tensor parallelism)
    - name: "apertus-70b-instruct"
      repository: "vllm/vllm-openai"
      tag: "v0.10.2"
      modelURL: "/models/Apertus-70B-sft-mixture-8e-aligned"  # Fixed: correct S3 mount path
      replicaCount: 1
      requestCPU: 60
      requestMemory: "600Gi"
      requestGPU: 8  # Factor of 64 for tensor parallelism
      
      vllmConfig:
        tensorParallelSize: 8

        extraArgs:
          - "--served-model-name"
          - "apertus-70b-instruct"
          - "--enable-auto-tool-choice"
          - "--tool-parser-plugin"
          - "/opt/apertus_tool_parser.py"
          - "--tool-call-parser"
          - "apertus_json"
          - "--chat-template"
          - "/opt/apertus_chat_template.jinja"
      
      extraVolumes:
        - name: s3-storage
          persistentVolumeClaim:
            claimName: s3-pvc
        - name: apertus-templates
          configMap:
            name: apertus-templates
            defaultMode: 0755

      extraVolumeMounts:
        - name: s3-storage
          mountPath: /models
        - name: apertus-templates
          mountPath: /opt/apertus_tool_parser.py
          subPath: apertus_tool_parser.py
        - name: apertus-templates
          mountPath: /opt/apertus_chat_template.jinja
          subPath: apertus_chat_template.jinja

    # Small model: 8B for testing (currently disabled)
    - name: "apertus-8b-instruct"
      repository: "vllm/vllm-openai"
      tag: "v0.10.2"
      modelURL: "/models/Apertus-8B-sft-mixture-8e-aligned"
      replicaCount: 0               # Disabled (set to 1 to enable)

      requestCPU: 16                # CPU cores per instance
      requestMemory: "64Gi"         # RAM per instance
      requestGPU: 8                 # GPUs per instance

      vllmConfig:
        tensorParallelSize: 8

        extraArgs:
          - "--served-model-name"
          - "apertus-8b-instruct"
          - "--enable-auto-tool-choice"
          - "--tool-parser-plugin"
          - "/opt/apertus_tool_parser.py"
          - "--tool-call-parser"
          - "apertus_json"
          - "--chat-template"
          - "/opt/apertus_chat_template.jinja"
      
      extraVolumes:
        - name: s3-storage
          persistentVolumeClaim:
            claimName: s3-pvc
        - name: apertus-templates
          configMap:
            name: apertus-templates
            defaultMode: 0755

      extraVolumeMounts:
        - name: s3-storage
          mountPath: /models
        - name: apertus-templates
          mountPath: /opt/apertus_tool_parser.py
          subPath: apertus_tool_parser.py
        - name: apertus-templates
          mountPath: /opt/apertus_chat_template.jinja
          subPath: apertus_chat_template.jinja

# Observability stack (optional)
observability:
  enabled: true  # Set to true to enable Prometheus/Grafana stack