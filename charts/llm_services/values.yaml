# Global values for llm-services chart
global:
  namespace: llm-services
  
  # AWS configuration
  aws:
    region: eu-central-2
    bucketName: publicai-bucket
    bucketPrefix: models/

# Karpenter GPU node pool configuration
nodePool:
  enabled: true
  name: gpu
  disruption:
    budgets:
    - nodes: "10%"
    consolidateAfter: 1h
    consolidationPolicy: WhenEmpty
  
  requirements:
    capacityType: "on-demand"  # Change to "capacity-block" for Capacity Blocks
    architecture: "amd64"
    instanceTypes:
      - "p4d.24xlarge"  # TODO: Adapt to p instances once available
  
  taints:
    - key: nvidia.com/gpu
      effect: NoSchedule
  
  terminationGracePeriod: 24h0m0s

# S3 storage configuration
s3Storage:
  enabled: true
  persistentVolume:
    name: s3-pv
    capacity: 1200Gi
    mountOptions:
      region: eu-central-2  # Will use global.aws.region if not specified
      prefix: models/      # Will use global.aws.bucketPrefix if not specified
    bucketName: publicai-bucket  # Will use global.aws.bucketName if not specified
  
  persistentVolumeClaim:
    name: s3-pvc
    capacity: 1200Gi

vllm:
  enabled: true

# Values for vllm-stack subchart  
vllm-stack:
  servingEngineSpec:
    runtimeClassName: ""
    vllmApiKey: "" # Will be set via --set from llm.sh script
    
    modelSpec:
    # Large model: 70B Apertus (4 GPUs - factor of 64 for tensor parallelism)
    - name: "apertus-70b-instruct"
      repository: "901779868336.dkr.ecr.eu-north-1.amazonaws.com/vllm-transformers-apertus"  # TODO: Fill in ECR registry (e.g., 123456789.dkr.ecr.eu-central-2.amazonaws.com/vllm-transformers-apertus)
      tag: "latest"
      modelURL: "/models/Apertus-70B-sft-mixture-8e-aligned"  # Fixed: correct S3 mount path
      replicaCount: 3
      requestCPU: 60
      requestMemory: "600Gi"
      requestGPU: 8  # Factor of 64 for tensor parallelism
      
      vllmConfig:
        tensorParallelSize: 8

        extraArgs:
          - "--served-model-name"
          - "apertus-70b-instruct"
      
      extraVolumes:
        - name: s3-storage
          persistentVolumeClaim:
            claimName: s3-pvc
      
      extraVolumeMounts:
        - name: s3-storage
          mountPath: /models
    
    # Small model: 8B or TinyLlama for testing (1 GPU)
    - name: "apertus-8b-instruct"
      repository: "901779868336.dkr.ecr.eu-north-1.amazonaws.com/vllm-transformers-apertus"
      tag: "latest"
      modelURL: "/models/Apertus-8B-sft-mixture-8e-aligned"
      replicaCount: 1
      
      requestCPU: 16
      requestMemory: "64Gi"
      requestGPU: 8

      vllmConfig:
        tensorParallelSize: 8
        extraArgs:
          - "--served-model-name"
          - "apertus-8b-instruct"
      
      extraVolumes:
        - name: s3-storage
          persistentVolumeClaim:
            claimName: s3-pvc
      
      extraVolumeMounts:
        - name: s3-storage
          mountPath: /models

# Observability stack (optional)
observability:
  enabled: true  # Set to true to enable Prometheus/Grafana stack